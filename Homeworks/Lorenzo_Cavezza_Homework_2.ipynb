{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> AstroStatistics and Cosmology - Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Due on 15$^{th}$ December 2024   -  Lorenzo Cavezza 2130648"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the characteristic function $\\phi$, for a given probability distribution, as $\\phi \\equiv \\mathbb{E} \\left[e^{-i \\boldsymbol{k x}}\\right]$. Therefore, $\\phi$ is the Fourier transform of the distribution:<br><br> <center>$\\normalsize \\phi \\boldsymbol(k) \\equiv \\int d^nx e^{-i \\boldsymbol{k \\cdot x}} p\\boldsymbol(x) $</center> <br>Show that the characteristic function of a multivariate Gaussian distribution, $N(\\mu,C)$, is equal to:\n",
    "<center>$\\normalsize \\phi \\boldsymbol(k) \\equiv e^{-i \\mu^T \\cdot \\boldsymbol{k} - \\frac{1}{2}\\boldsymbol{k}^TC\\boldsymbol{k}} $</center><br>\n",
    "Do this in two different ways. First. Start from the original multivariate Gaussian and perform the Fourier transform by completing the square in the integrand. Second: perform a suitable rotation to diagonalize the covariance, and transform the resulting distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Given:<center>\n",
    "$N(\\boldsymbol{x} | \\boldsymbol{\\mu},C) = \\frac{1}{\\sqrt{ (2\\pi)^n \\det{C}}}e^{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^TC^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})}$ </center>we get:\n",
    "<center>\n",
    "$\\small \\phi \\boldsymbol(k) \\equiv \\int d^nx e^{-i \\boldsymbol{k \\cdot x}} N(x | \\mu,C) = \\normalsize \n",
    "\\frac{1}{\\sqrt{ (2\\pi)^n \\det{C}}} \\int d^nx e^{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^TC^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})-i \\boldsymbol{k \\cdot x}}=\n",
    "\\frac{1}{\\sqrt{ (2\\pi)^n \\det{C}}} \\int d^nx e^{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^TC^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})-i \\boldsymbol{k \\cdot x}+i \\boldsymbol{k \\cdot \\mu}-i \\boldsymbol{k \\cdot \\mu}} =$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$\n",
    "\\frac{1}{\\sqrt{ (2\\pi)^n \\det{C}}} \\int d^nx e^{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^TC^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})-i \\boldsymbol{k^T} (\\boldsymbol{x} - \\boldsymbol{\\mu})-i \\boldsymbol{\\mu \\cdot k}} =\n",
    "\\left[1\\right]= \n",
    "\\frac{1}{\\sqrt{ (2\\pi)^n \\det{C}}} \\int d^nx e^{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}+iCk)^TC^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}+iCk)-\\frac{1}{2} \\boldsymbol{k^T}C\\boldsymbol{k} -i \\boldsymbol{\\mu^T k}}=\n",
    "\\left[2\\right]= \n",
    "e^{-\\frac{1}{2} \\boldsymbol{k^T}C\\boldsymbol{k} -i \\boldsymbol{\\mu^T \\cdot k}}$</center> \n",
    "\n",
    "Where in $\\left[1\\right]$ we exploited the square completion formula while in $\\left[2\\right]$ we made use of the fact that the first term in the exponential is just that of a Gaussian with the same precision matrix as $N(x|\\mu,C)$ and slightly different mean. Since the normalization is C dependent only its integral is exactly equal to the original normalization leading to them being crossed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** We perform a change of variable that allows us to perform a rotation that diagonalize the covariance $\\boldsymbol{x}=O\\boldsymbol{y}$ with $O^TO=\\mathbb{1}$ and $O^TC^{-1}O=D$ with $D$ diagonal:\n",
    "<center>$\\small \\phi \\boldsymbol(k) \\equiv \\int d^nx e^{-i \\boldsymbol{k \\cdot x}} N(x | \\mu,C) =\\normalsize \n",
    "\\frac{1}{\\sqrt{ (2\\pi)^n \\det{C}}} \\int d^nx e^{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^TC^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})-i \\boldsymbol{k \\cdot x}}=\\normalsize \n",
    "\\frac{1}{\\sqrt{ (2\\pi)^n \\det{C}}} \\int d^nx e^{-\\frac{1}{2}\\boldsymbol{x}^TC^{-1}\\boldsymbol{x}-i \\boldsymbol{k} \\cdot (\\boldsymbol{x}+\\boldsymbol{\\mu})}=$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$=\\left[\\boldsymbol{x}=O\\boldsymbol{y}\\right]=\\normalsize \n",
    "\\frac{1}{\\sqrt{ (2\\pi)^n \\det{C}}} \\int d^ny  e^{-\\frac{1}{2}\\boldsymbol{y}^TO^TC^{-1}O\\boldsymbol{y}-i \\boldsymbol{k} \\cdot (O\\boldsymbol{y}+\\boldsymbol{\\mu})}\\det{(O)}=\\frac{1}{\\sqrt{ (2\\pi)^n \\det{C}}} \\int d^ny e^{-\\frac{1}{2}\\sum_i(d_iy_i^2-e_iy_i) -\\boldsymbol{k}^T\\boldsymbol{\\mu}}=$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$=\\frac{e^{-\\boldsymbol{k}^T\\boldsymbol{\\mu}}}{\\sqrt{ (2\\pi)^n}}\\prod_i\\int d^nyd_ie^{-\\left(\\frac{\\sqrt{2d_i}}{2}y_i+\\sqrt{\\frac{2}{d_i}}\\frac{e_i}{2}\\right)^2+\\frac{e_i^2}{2d_i}}=\\left[\\frac{\\sqrt{2d_i}}{2}y_i+\\sqrt{\\frac{2}{d_i}}\\frac{e_i}{2}=\\frac{z_i}{\\sqrt{2}}\\right]=\\frac{e^{-\\boldsymbol{k}^T\\boldsymbol{\\mu}}}{\\sqrt{ (2\\pi)^n}}\\prod_i\\int d^nz\\sqrt{d_i}e^{-\\frac{z_i^2}{2}+\\frac{e_i^2}{2d_i}}=$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$=\\frac{e^{-\\boldsymbol{k}^T\\boldsymbol{\\mu}}}{\\sqrt{ (2\\pi)^n}}\\prod_i\\sqrt{d_i}\\sqrt{\\frac{2\\pi}{d_i}}e^{\\frac{e_i^2}{2d_i}}=e^{-\\boldsymbol{k}^T\\boldsymbol{\\mu}}\\prod_ie^{\\frac{e_i^2}{2d_i}}=e^{-\\boldsymbol{k}^T\\boldsymbol{\\mu}}e^{-\\frac{\\boldsymbol{k}^TOD^{-1}O^T\\boldsymbol{k}}{2}}=e^{ -i \\boldsymbol{\\mu^T k}-\\frac{1}{2} \\boldsymbol{k^T}C\\boldsymbol{k}}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characteristic function is useful because it can be used to generate moments of the distribution via differentiation.\n",
    "<center>$\\large \\mathbb{E}\\left[x_{\\alpha}^{n_{\\alpha}}...x_{\\beta}^{n_{\\beta}}\\right] = \\left[\\frac{\\partial^{n_{\\alpha}...n_{\\beta}}\\phi (\\boldsymbol{k})}{\\partial \\left(-ik_{\\alpha}\\right)^{n_{\\alpha}}...\\partial\\left(-ik_{\\beta}\\right)^{n_{\\beta}}}\\right]_{\\boldsymbol{k}=0} $</center>\n",
    "Apply this to a Multivariate Gaussian, $N(\\mu,\\Sigma)$, to find its mean and covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the characteristic function found above: $\\phi(\\boldsymbol{k})=e^{-\\frac{1}{2} \\boldsymbol{k^T}C\\boldsymbol{k} -i \\boldsymbol{\\mu^T \\cdot k}}$ we take the first and second derivative with respect to k:<br><br><center>\n",
    "$\\boldsymbol{\\hat\\mu}=\\mathbb{E}\\left[x\\right]=\\frac{1}{-i}\\nabla_k\\phi (\\boldsymbol{k})\\big|_{\\boldsymbol{k}=0}=i \\nabla_k\\left(-\\frac{1}{2} \\boldsymbol{k^T}C\\boldsymbol{k} -i \\boldsymbol{\\mu^T \\cdot k}\\right)\\big|_{\\boldsymbol{k}=0}=i \\left(-\\frac{1}{2} 2C\\boldsymbol{k} -i \\boldsymbol{\\mu}\\right)\\big|_{\\boldsymbol{k}=0}=\\left(-i C\\boldsymbol{k} + \\boldsymbol{\\mu}\\right)\\big|_{\\boldsymbol{k}=0}=\\boldsymbol{\\mu}$</center>\n",
    "<center>\n",
    "$\\boldsymbol{\\hat\\sigma}=\\mathbb{E}\\left[x^2\\right]=-\\frac{\\partial^2\\phi (\\boldsymbol{k})}{\\partial k_i\\partial k_j}\\big|_{\\boldsymbol{k}=0}=-\\frac{\\partial\\left(-i C\\boldsymbol{k} + \\boldsymbol{\\mu}\\right)}{i\\partial k_j}\\big|_{\\boldsymbol{k}=0}=C\n",
    " $</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a data vector $\\vec{d}$ of length $N$. We want to fit the data using a linear combination of $M$ templates $\\vec{t}$ with unknown amplitudes $\\vec{A}$, i.e. we assume that the average of the data is given by:\n",
    "<center>$\\normalsize \\langle \\vec{d}\\rangle = \\sum_{i=1}^{M} A_i\\vec{t}_i \\equiv \\vec{A}^T \\cdot \\vec{T}$</center>\n",
    "If we further assume that the data are Gaussian distributed with covariance $C$, the best-fit amplitude parameter vector is found via minimization of the chi-squared statistics:\n",
    "<center>$\\normalsize \\chi^2 = \\left(\\vec{d} - \\vec{A}^TT\\right)^TC^{-1}\\left(\\vec{d}-\\vec{A}^TT\\right)$</center>\n",
    "Perform such minimization to get an estimate of $\\vec{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$\\normalsize \\chi^2 = \n",
    "\\left(\\vec{d} - \\vec{A}^TT\\right)^TC^{-1}\\left(\\vec{d}-\\vec{A}^TT\\right)  = \n",
    "\\left(d^{\\alpha}-A_{\\beta}T^{\\beta\\alpha}\\right)C^{-1}_{\\alpha\\gamma}\\left(d^{\\gamma}-A_{\\delta}T^{\\delta\\gamma}\\right) \n",
    "= d^{\\alpha}C^{-1}_{\\alpha\\gamma}d^{\\gamma} - 2 d^{\\alpha}C^{-1}_{\\alpha\\gamma}A_{\\delta}T^{\\delta\\gamma}+A_{\\beta}T^{\\beta\\alpha}C^{-1}_{\\alpha\\gamma}A_{\\delta}T^{\\delta\\gamma}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the derivative with respect to $A$ and set it to $0$:<br><br>\n",
    "<center>$\\vec{0}=\\nabla_A\\chi^2=\\frac{\\partial\\chi^2}{\\partial A_k} = \n",
    "0^{k}- 2 d^{\\alpha}C^{-1}_{\\alpha\\gamma}\\delta^{k}_{\\delta}T^{\\delta\\gamma}+A_{\\beta}T^{\\beta\\alpha}C^{-1}_{\\alpha\\gamma}\\delta^{k}_{\\delta}T^{\\delta\\gamma}=\n",
    "- 2 d^{\\alpha}C^{-1}_{\\alpha\\gamma}T^{k\\gamma}+A_{\\beta}T^{\\beta\\alpha}C^{-1}_{\\alpha\\gamma}T^{k\\gamma}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to: <br>\n",
    "<center>$d^{\\alpha}C^{-1}_{\\alpha\\gamma}T^{k\\gamma}=\n",
    "A_{\\beta}T^{\\beta\\alpha}C^{-1}_{\\alpha\\gamma}T^{k\\gamma}\\implies dC^{-1}T^T=A^TTC^{-1}T^T\\implies A^T = dC^{-1}T^T\\left(TC^{-1}T^T\\right)^{-1}\\implies A=\\left(TC^{-1}T^T\\right)^{-1}TC^{-1}d^T$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have it being a minimum we should have:<br><br>\n",
    "<center>$\\frac{\\partial^2\\chi^2}{\\partial A_k\\partial A_j}=\n",
    "\\frac{\\partial}{\\partial A_j}\\left(- 2 d^{\\alpha}C^{-1}_{\\alpha\\gamma}T^{k\\gamma}+A_{\\beta}T^{\\beta\\alpha}C^{-1}_{\\alpha\\gamma}T^{k\\gamma}\\right)=\n",
    "0^{kj}+\\delta_{\\beta}^jT^{\\beta\\alpha}C^{-1}_{\\alpha\\gamma}T^{k\\gamma}=\n",
    "T^{j\\alpha}C^{-1}_{\\alpha\\gamma}T^{k\\gamma}\\geq0^{kj}$</center><br>\n",
    "Which is positive since $C^{-1}$ is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a dataset $\\vec{d} = \\left(d_1,...,d_N\\right)$, where the measurement $d_i$ is taken at time/position $x_i$. The average of $\\vec{d}$ is assumed to be $\\langle \\vec{d}\\rangle = \\omega \\vec{x} + b$ (i.e., you are going to fit the data with a straight line). Noise is Gaussian and uncorrelated. Find the MAP estimate of parameters $\\omega, b$ and the parameter covariance matrix, assuming a uniform prior (note: you can take the result of the previous exercise as a starting point; of course the result yields the usual linear regression formulae)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\langle\\vec{d}\\rangle=\\omega\\vec{x}+b\\vec{1}=\\vec{A}^TT$ where $\\vec{A}=\\begin{bmatrix}\n",
    "\\omega \\\\ b\\end{bmatrix}$ and $T=\\begin{bmatrix} \\vec{x} \\\\\\vec{1}\\end{bmatrix}$.<br>\n",
    "We get that:\n",
    "<center>$P(\\omega,b|\\vec{d})\\propto P(\\vec{d}|\\omega,b)P(\\omega,b)\\propto P(\\vec{d}|\\omega,b)$</center><br> since the prior is uniform and can be considered as a constant improper prior for the purpose of the following computations. Since the noise is Gaussian and uncorrelated the covariance matrix is $\\Sigma=diag(\\sigma^2_1,...,\\sigma^2_n)$ and the likelihood therefore takes the form of a MVN:<br><br>\n",
    "<center>$P(\\vec{d}|\\omega,b)=\\frac{1}{\\sqrt{(2\\pi)^{n}\\det{\\Sigma}}}e^{-\\frac{1}{2}(\\vec{d}-\\langle\\vec{d}\\rangle)^T\\Sigma^{-1}(\\vec{d}-\\langle\\vec{d}\\rangle)}=\\frac{1}{\\sqrt{(2\\pi)^{n}\\det{\\Sigma}}}e^{-\\frac{1}{2}(\\vec{d}-\\omega\\vec{x}-b)^T\\Sigma^{-1}(\\vec{d}-\\omega\\vec{x}-b)}$</center><br>Determining the MAP involves maximising such likelihood with respect to the parameters $\\vec{A}$  and therefore minimizing $\\small(\\vec{d}-\\omega\\vec{x}-b)^T\\Sigma^{-1}(\\vec{d}-\\omega\\vec{x}-b)=\\chi^2$. The minimum of $\\chi^2$ with respect to $\\vec{A}$ was determined to be: <br><br>\n",
    "<center>$\\small A=\\left(T\\Sigma^{-1}T^T\\right)^{-1}T\\Sigma^{-1}d^T=\\left(\\normalsize\\frac{1}{det(\\Sigma)}\\small T\\tiny\\begin{bmatrix}\\sigma^{-2}_{1} & & \\\\ & \\ddots & \\\\ & & \\sigma^{-2}_{n}\\end{bmatrix}\\small T^T\\right)^{-1}\\normalsize \\frac{1}{det(\\Sigma)}T\\tiny\\begin{bmatrix}\\sigma^{-2}_{1} & & \\\\ & \\ddots & \\\\ & & \\sigma^{-2}_{n}\\end{bmatrix}\\small d^T$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$=\\small\\left(T\\tiny\\begin{bmatrix}\\sigma^{-2}_{1} & & \\\\ & \\ddots & \\\\ & & \\sigma^{-2}_{n}\\end{bmatrix}\\small T^T\\right)^{-1}T\\tiny\\begin{bmatrix}\\sigma^{-2}_{1} & & \\\\ & \\ddots & \\\\ & & \\sigma^{-2}_{n}\\end{bmatrix}\\small d^T=\\left(\\begin{bmatrix} \\vec{x} \\\\\\vec{1}\\end{bmatrix} \\tiny\\begin{bmatrix}\\sigma^{-2}_{1} & & \\\\ & \\ddots & \\\\ & & \\sigma^{-2}_{n}\\end{bmatrix}\\small\\begin{bmatrix} \\vec{x} &\\vec{1}\\end{bmatrix}\\right)^{-1}\\begin{bmatrix} \\vec{x} \\\\\\vec{1}\\end{bmatrix}\\tiny\\begin{bmatrix}\\sigma^{-2}_{1} & & \\\\ & \\ddots & \\\\ & & \\sigma^{-2}_{n}\\end{bmatrix}\\small\\begin{bmatrix} d_1 ...d_n\\end{bmatrix}=$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$=\\small\\left(\\begin{bmatrix} x_1\\sigma_1^{-2} &.....&x_n\\sigma_n^{-2}\\\\\\sigma^{-2}_1&.....&\\sigma^{-2}_n\\end{bmatrix}\\begin{bmatrix} \\vec{x} &\\vec{1}\\end{bmatrix}\\right)^{-1}\\begin{bmatrix} x_1\\sigma_1^{-2} &.....&x_n\\sigma_n^{-2}\\\\\\sigma^{-2}_1&.....&\\sigma^{-2}_n\\end{bmatrix}\\begin{bmatrix} d_1 ...d_n\\end{bmatrix}=\\begin{bmatrix} \\sum_i x_i^2\\sigma_i^{-2} &\\sum_i x_i\\sigma_i^{-2}\\\\\\sum_i x_i\\sigma^{-2}_i&\\sum_i \\sigma^{-2}_i\\end{bmatrix}^{-1}\\begin{bmatrix} \\sum_i x_id_i\\sigma_i^{-2} \\\\\\sum_i d_i\\sigma^{-2}_i\\end{bmatrix}=$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$=\\small\\frac{1}{\\sum_i x_i^2\\sigma_i^{-2}\\sum_i \\sigma^{-2}_i-\\left(\\sum_i x_i\\sigma_i^{-2}\\right)^2}\\begin{bmatrix}\\sum_i \\sigma^{-2}_i\\sum_i x_id_i\\sigma_i^{-2} -\\sum_i x_i\\sigma_i^{-2}\\sum_i d_i\\sigma^{-2}_i\\\\-\\sum_i x_i\\sigma^{-2}_i\\sum_i x_id_i\\sigma_i^{-2}+ \\sum_i x_i^2\\sigma_i^{-2}\\sum_i d_i\\sigma^{-2}_i\\end{bmatrix}=\\begin{bmatrix}\n",
    "\\hat{\\omega} \\\\ \\hat{b}\\end{bmatrix}$</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the covariance goes we need to evaluate the Hessian of the posterior in such maximum and exploit the fact that $cov=-H^{-1}$; we had already computed the second partial derivative with respect to A of $\\chi^2$ (which is proportional to the same derivative of the posterior) and we had found that:<br><br>\n",
    "<center>$\\normalsize\\frac{\\partial^2\\chi^2}{\\partial A_k\\partial A_j}=\n",
    "T^{j\\alpha}C^{-1}_{\\alpha\\gamma}T^{k\\gamma}=TC^{-1}T^T\\implies\\small Cov(\\vec{A})=-(T\\Sigma^{-1}T^T)^{-1}=-\\begin{bmatrix} \\sum_i x_i^2\\sigma_i^{-2} &\\sum_i x_i\\sigma_i^{-2}\\\\\\sum_i x_i\\sigma^{-2}_i&\\sum_i \\sigma^{-2}_i\\end{bmatrix}^{-1}=$<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$=\\frac{1}{\\sum_i x_i^2\\sigma_i^{-2}\\sum_i \\sigma^{-2}_i-\\left(\\sum_i x_i\\sigma_i^{-2}\\right)^2}\\begin{bmatrix}-\\sum_i \\sigma^{-2}_i &\\sum_i x_i\\sigma_i^{-2}\\\\\\sum_i x_i\\sigma^{-2}_i& -\\sum_i x_i^2\\sigma_i^{-2}\\end{bmatrix}$\n",
    "    </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

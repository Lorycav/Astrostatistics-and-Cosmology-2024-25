{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> AstroStatistics and Cosmology - Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Due on $7^{th}$ January 2025   -  Lorenzo Cavezza 2130648"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a dataset $(d_1, . . . , d_n)$, where the measurement $d_i$ is taken at time/position $\\vec{x_i}$ . The average of $d_i$ is assumed to be $\\small d_i=\\beta_0+\\sum^p_{j=1}\\beta_jx_{ij}$. Noise is Gaussian and uncorrelated, therefore $d_i =  \\beta_0 + \\sum_{j=1}^p \\beta_{j}x_{ij}+\\epsilon_i$ where $\\epsilon \\sim N(0,\\sigma^2)$. Assuming uniform priors on the parameters  $\\beta_i$ ($i = 0, . . . , p$), it is well known and easy to see that the MAP estimate of $\\small\\vec{\\beta} = (\\beta_0, ...,\\beta_p )$ is found via least square fitting, i.e., by minimizing the function $\\small RSS \\equiv \\sum_{i=1}^n \\left(d_i-\\beta_0-\\sum_{j=1}^p \\beta_j x_{ij}\\right)^2$ ($RSS$: “residual sum of squares”). We want to see how this result is modified by different, specific choices of prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Assume the following prior for $\\vec{\\beta}$: all $\\beta_i$ are i.i.d., according to a double exponential (Laplace) distribution: $p(\\beta_i)=\\frac{1}{2b}e^{-\\frac{\\lvert\\beta_i\\rvert}{b}}$, where $b$ is some fixed scale parameter. Show in this case that MAP estimation leads to the following solution (“LASSO regression”):<center>\n",
    "$\\hat{\\vec{\\beta}}=\\underset{\\vec{\\beta}}{\\operatorname{argmin}}\\left(RSS +\\frac{2\\sigma^2}{b}\\sum_{j=1}^p\\lvert\\beta_j\\rvert\\right)$\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dataset we can define the likelihood as:<center>\n",
    "$P\\left(\\vec{d}|\\vec{\\beta}\\right)= \\prod_{i=1}^n N\\left(\\beta_0+\\sum^p_{j=1}\\beta_jx_{ij},\\sigma^2\\right)\\propto \\prod_{i=1}^nexp\\left(-\\frac{\\left(d_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij}\\right)^2}{2\\sigma^2}\\right)$</center>\n",
    "Then given Laplace priors we get the posterior:<center>\n",
    "$P\\left(\\vec{\\beta}|\\vec{d}\\right)\\propto\\prod_{i=1}^nexp\\left(-\\frac{\\left(d_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij}\\right)^2}{2\\sigma^2}\\right)\\prod_{j=1}^p\\frac{1}{2b}e^{-\\frac{\\lvert\\beta_j\\rvert}{b}}$</center>Which we want to maximise to find the MAP; before doing it we apply a log, which doens't change the MAP since it's monotonically increasing:<center>\n",
    "$L=log{\\left(P\\left(\\vec{\\beta}|\\vec{d}\\right)\\right)}\\propto-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\left(d_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij}\\right)^2-\\sum_{j=1}^p\\frac{\\lvert\\beta_j\\rvert}{b}=-\\frac{RSS}{2\\sigma^2}-\\frac{1}{b}\\sum_{j=1}^p\\lvert\\beta_j\\rvert$\n",
    "    </center>\n",
    "Therefore: <center>\n",
    "    $\\hat{\\vec{\\beta}}=\\underset{\\vec{\\beta}}{\\operatorname{argmax}}(L)=\\underset{\\vec{\\beta}}{\\operatorname{argmax}}(2\\sigma^2L)=\\underset{\\vec{\\beta}}{\\operatorname{argmax}}\\left(-RSS -\\frac{2\\sigma^2}{b}\\sum_{j=1}^p\\lvert\\beta_j\\rvert\\right)=\\underset{\\vec{\\beta}}{\\operatorname{argmin}}\\left(RSS +\\frac{2\\sigma^2}{b}\\sum_{j=1}^p\\lvert\\beta_j\\rvert\\right)$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Assume the following prior for $\\vec{\\beta}$: all $\\beta_i$ are i.i.d., according to a Gaussian distribution with mean $0$ and variance $c$. Show in this case that MAP estimation leads instead to the following solution (“ridge regression”):\n",
    "<center>\n",
    "    $\\hat{\\vec{\\beta}}=\\underset{\\vec{\\beta}}{\\operatorname{argmin}}\\left(RSS +\\frac{\\sigma^2}{c}\\sum_{j=1}^p\\beta_j^2\\right)$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the same likelihood:<center>\n",
    "$P\\left(\\vec{d}|\\vec{\\beta}\\right)= \\prod_{i=1}^n N\\left(\\beta_0+\\sum^p_{j=1}\\beta_jx_{ij},\\sigma^2\\right)\\propto \\prod_{i=1}^nexp\\left(-\\frac{\\left(d_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij}\\right)^2}{2\\sigma^2}\\right)$</center>\n",
    "We consider a normal prior:<center>\n",
    "$P\\left(\\vec{\\beta}|\\vec{d}\\right)\\propto\\prod_{i=1}^nexp\\left(-\\frac{\\left(d_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij}\\right)^2}{2\\sigma^2}\\right)\\prod_{j=1}^pexp\\left(-\\frac{\\beta_j^2}{2c}\\right)$</center>which we want to maximise to find the MAP; before doing so we apply a log, which doens't change the MAP since it's monotonically increasing, and we neglect some constant terms:<center>\n",
    "$L=log{\\left(P\\left(\\vec{\\beta}|\\vec{d}\\right)\\right)}\\propto-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\left(d_i-\\beta_0-\\sum^p_{j=1}\\beta_jx_{ij}\\right)^2-\\sum_{j=1}^p\\frac{\\beta_j^2}{2c}=-\\frac{RSS}{2\\sigma^2}-\\frac{1}{2c}\\sum_{j=1}^p\\beta_j^2$\n",
    "    </center>\n",
    "Therefore: <center>\n",
    "    $\\hat{\\vec{\\beta}}=\\underset{\\vec{\\beta}}{\\operatorname{argmax}}(L)=\\underset{\\vec{\\beta}}{\\operatorname{argmax}}(2\\sigma^2L)=\\underset{\\vec{\\beta}}{\\operatorname{argmax}}\\left(-RSS -\\frac{2\\sigma^2}{2c}\\sum_{j=1}^p\\beta_j^2\\right)=\\underset{\\vec{\\beta}}{\\operatorname{argmin}}\\left(RSS +\\frac{\\sigma^2}{c}\\sum_{j=1}^p\\beta_j^2\\right)$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a likelihood with the average depending on some amplitude A, just rescaling the mean:\n",
    "<center>\n",
    "    $L(\\Theta,x)=\\frac{1}{\\sqrt{(2\\pi)^N\\left\\lvert C\\right\\rvert}}e^{-\\frac{1}{2}\\left(x-A\\hat{x}\\right)^TC^{-1}\\left(x-A\\hat{x}\\right)}$\n",
    "</center>\n",
    "In the above $\\hat{x}$ is the theoretical expectation value, depending on the parameter vector $\\Theta$, in which we set the amplitude parameter $A$ to $A = 1$. Marginalize over $A$ assuming a uniform (unnormalized) $p(A) = 1$ prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the prior is uniform the posterior is proportional to the likelihood, so we need to compute:<center>\n",
    "$\\frac{1}{\\sqrt{(2\\pi)^N\\left\\lvert C\\right\\rvert}}\\int dA \\exp{\\left(-\\frac{1}{2}\\left(x-A\\hat{x}\\right)^TC^{-1}\\left(x-A\\hat{x}\\right)\\right)}=\\frac{1}{\\sqrt{(2\\pi)^N\\left\\lvert C\\right\\rvert}}\\int dA \\exp{\\left[\\frac{1}{2}\\left(-\\hat{x}^TC^{-1}\\hat{x}A^2+x^TC^{-1}\\hat{x}A+\\hat{x}^TC^{-1}\\hat{x}A-x^TC^{-1}x\\right)\\right]}$</center>\n",
    "<center>\n",
    "$=\\frac{1}{\\sqrt{(2\\pi)^N\\left\\lvert C\\right\\rvert}}\\int dA \\exp{\\left[-\\frac{1}{2}\\hat{x}^TC^{-1}\\hat{x}A^2+x^TC^{-1}\\hat{x}A-x^TC^{-1}x\\right]}=$[Completing the square]=</center><center>$=\\frac{1}{\\sqrt{(2\\pi)^N\\left\\lvert C\\right\\rvert}}\\exp{\\left[\\frac{\\left(x^TC^{-1}\\hat{x}\\right)^2}{2\\hat{x}^TC^{-1}\\hat{x}}-\\frac{1}{2}x^TC^{-1}x\\right]}\\int dA \\exp{\\left[-\\frac{1}{2}\\left(\\sqrt{\\hat{x}^TC^{-1}\\hat{x}}A-\\frac{x^TC^{-1}\\hat{x}}{\\sqrt{\\hat{x}^TC^{-1}\\hat{x}}}\\right)\\right]}=\\tiny\\left[\\sqrt{\\hat{x}^TC^{-1}\\hat{x}}A-\\frac{x^TC^{-1}\\hat{x}}{\\sqrt{\\hat{x}^TC^{-1}\\hat{x}}}=y \\implies\\sqrt{\\hat{x}^TC^{-1}\\hat{x}}dA=dy\\right]=$  \n",
    "</center><center>\n",
    "$=\\frac{1}{\\sqrt{\\hat{x}^TC^{-1}\\hat{x}}\\sqrt{(2\\pi)^N\\left\\lvert C\\right\\rvert}}\\exp{\\left[\\frac{\\left(x^TC^{-1}\\hat{x}\\right)^2}{2\\hat{x}^TC^{-1}\\hat{x}}-\\frac{1}{2}x^TC^{-1}x\\right]}\\int dy \\exp{\\left(-\\frac{1}{2}y^2\\right)}=\\sqrt{\\frac{2\\pi}{\\hat{x}^TC^{-1}\\hat{x}(2\\pi)^N\\left\\lvert C\\right\\rvert}}\\exp{\\left[\\frac{\\left(x^TC^{-1}\\hat{x}\\right)^2}{2\\hat{x}^TC^{-1}\\hat{x}}-\\frac{1}{2}x^TC^{-1}x\\right]}$\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have collected a data time series $D = \\{d_1\\equiv d(t_1), d_2\\equiv d(t_2), . . . , d_n\\equiv d(t_N)\\}$. Your data are evenly time spaced ($t_{i+1}-t_i = \\Delta$ for any $i,1\\leq i\\geq N$ ). You assume that the time series contains a single stationary sinusoidal signal $f (t)$ plus noise. You also assume that noise is Gaussian with known variance $\\sigma^2$, and that different measurements are statistically independent. Your data are therefore modeled as\n",
    "<center>\n",
    "  $d_i=f(t_i)+n_i=B_1\\cos{\\omega t_i}+B_2\\sin{\\omega t_i}+n_i$  \n",
    "</center>\n",
    "where $B_1$ and $B_2$ are unknown amplitudes, $\\omega$ is the unknown frequency of the signal, and $n_i$ are the noise contributions. In the following, always assume to work in a large frequency limit for your signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show that the likelihood $L(D|\\omega, B_1 , B_2 , I)$ can be written in the form:<center>\n",
    "$L=(2\\pi\\sigma^2)^{-\\frac{N}{2}}e^{-\\frac{Q}{2\\sigma^2}}$</center><center>\n",
    "$Q\\equiv N\\hat{d}^2-2\\left[B_1R(\\omega)+B_2I(\\omega)\\right]+B_1^2c+B_2^2s$</center><center>\n",
    "$R(\\omega)=\\sum_{i=1}^Nd_i\\cos{\\omega t_i}$</center><center>\n",
    "$I(\\omega)=\\sum_{i=1}^Nd_i\\sin{\\omega t_i}$</center><center>\n",
    "$c\\equiv\\sum_{i=1}^N\\cos^2{\\omega t_i}$</center><center>\n",
    "$s\\equiv\\sum_{i=1}^N\\sin^2{\\omega t_i}$</center>\n",
    "where $\\hat{d}^2$ is the arithmetic average of the square of the data in the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is gaussian and independently distributed around the sinusoidal mean the likelihood is:<center>\n",
    "$L=\\prod_{i=1}^N(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\left(d_i-\\hat{d}\\right)^2\\right]}=(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp{\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N\\left(d_i-B_1\\cos{\\omega t_i}-B_2\\sin{\\omega t_i}\\right)^2\\right]}=$</center><center>$=(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N\\left[d_i^2+B_1^2\\cos^2{\\omega t_i}+B_2^2\\sin^2{\\omega t_i}-2d_i\\left(B_1\\cos{\\omega t_i}+B_2\\sin{\\omega t_i}\\right)-2B_1B_2\\cos{\\omega t_i}\\sin{\\omega t_i}\\right]\\right\\}}=$</center><center>\n",
    "    $=(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^N\\left[d_i^2\\right]+B_1^2\\sum_{i=1}^N\\left[\\cos^2{\\omega t_i}\\right]+B_2^2\\sum_{i=1}^N\\left[\\sin^2{\\omega t_i}\\right]-2\\sum_{i=1}^N\\left[d_i\\left(B_1\\cos{\\omega t_i}+B_2\\sin{\\omega t_i}\\right)\\right]-B_1B_2\\sum_{i=1}^N \\sin{2\\omega t_i}\\right)\\right\\}}=$</center>\n",
    "<center>\n",
    "    $=[1]=(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\left(N\\frac{\\sum_{i=1}^N\\left[d_i^2\\right]}{N}+B_1^2\\sum_{i=1}^N\\left[\\cos^2{\\omega t_i}\\right]+B_2^2\\sum_{i=1}^N\\left[\\sin^2{\\omega t_i}\\right]-2\\left(B_1\\sum_{i=1}^N\\left[d_i\\cos{\\omega t_i}\\right]+B_2\\sum_{i=1}^N\\left[d_i\\sin{\\omega t_i}\\right]\\right)\\right)\\right\\}}=$</center><center>\n",
    "    $=(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\left(N\\hat{d}^2+B_1^2c+B_2^2s-2\\left[B_1R\\left(\\omega\\right)+B_2I\\left(\\omega\\right)\\right]\\right)\\right\\}}=(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}Q\\right\\}}$</center><br>\n",
    "where in $1$ we approximate $B_1B_2\\sum_{i=1}^N \\sin{2\\omega t_i}$ to be $0$ since the $\\sin$ is changing rapidly around zero and averages out to that value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show that $s$ and $c$, defined above, are well approximated as $\\omega$-independent constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that c and s are $\\omega$-independent we derive them with respect to $\\omega$ and show that such derivatives tend to zero as $\\omega$ tends to infinity:\n",
    "<center>\n",
    "$\\frac{\\partial c}{\\partial \\omega}=\\frac{\\partial \\left(\\sum_{i=1}^N\\cos^2{\\omega t_i}\\right)}{\\partial \\omega}=\\sum_{i=1}^N\\frac{\\partial \\left(\\cos^2{\\omega t_i}\\right)}{\\partial \\omega}=-\\sum_{i=1}^N2t_i\\sin{\\omega t_i}\\cos{\\omega t_i}=-\\sum_{i=1}^Nt_i\\sin{2\\omega t_i}$</center><center>\n",
    "    $\\frac{\\partial s}{\\partial \\omega}=\\frac{\\partial \\left(\\sum_{i=1}^N\\sin^2{\\omega t_i}\\right)}{\\partial \\omega}=\\sum_{i=1}^N\\frac{\\partial \\left(\\sin^2{\\omega t_i}\\right)}{\\partial \\omega}=\\sum_{i=1}^N2t_i\\sin{\\omega t_i}\\cos{\\omega t_i}=\\sum_{i=1}^Nt_i\\sin{2\\omega t_i}$\n",
    "    \n",
    "</center>\n",
    "As $\\omega$ tends to a high value the $\\sin$ oscillates very rapidly between $-1$ and $1$ averaging out to about zero in the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider uniform (proper or improper) priors on the three parameters of the problem ($\\omega, B_1 , B_2$). Treating the amplitudes $B_1$ and $B_2$ as nuisance parameters, find the marginalized 1-parameter posterior for $\\omega$. Show that the marginalized posterior can be written as:<center>$\n",
    "P(\\omega|D, I ) \\propto \\frac{1}{\\sqrt{cs}}exp\\left[\\frac{R(\\omega)^2/c+I(\\omega)^2/s}{2\\sigma^2}\\right]$\n",
    "</center>(note that marginalization amounts to solving Gaussian integrals, which can be done analytically via standard techniques)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the priors are uniform the posterior is proportional to the likelihood so:<center>\n",
    "$P(\\omega|D, I ) \\propto \\int dB_1dB_2(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\left(N\\hat{d}^2+B_1^2c+B_2^2s-2\\left[B_1R\\left(\\omega\\right)+B_2I\\left(\\omega\\right)\\right]\\right)\\right\\}}=$\n",
    "</center><center>\n",
    "$=(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}N\\hat{d}^2\\right\\}}\\int dB_1\\exp{\\left\\{-\\frac{c}{2\\sigma^2}\\left(B_1^2-2B_1\\frac{R\\left(\\omega\\right)}{c}\\right)\\right\\}}\\int dB_2\\exp{\\left\\{-\\frac{s}{2\\sigma^2}\\left(B_2^2-2B_2\\frac{I\\left(\\omega\\right)}{s}\\right)\\right\\}}=$</center><center>\n",
    "$=(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\left(N\\hat{d}^2-\\frac{R^2\\left(\\omega\\right)}{c}-\\frac{I^2\\left(\\omega\\right)}{s}\\right)\\right\\}}\\int dB_1\\exp{\\left\\{-\\frac{c}{2\\sigma^2}\\left(B_1-\\frac{R\\left(\\omega\\right)}{c}\\right)^2\\right\\}}\\int dB_2\\exp{\\left\\{-\\frac{s}{2\\sigma^2}\\left(B_2-\\frac{I\\left(\\omega\\right)}{s}\\right)^2\\right\\}}=$\n",
    "    </center><center>\n",
    "$=(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\left(N\\hat{d}^2-\\frac{R^2\\left(\\omega\\right)}{c}-\\frac{I^2\\left(\\omega\\right)}{s}\\right)\\right\\}}2\\pi\\sigma^2\\left(cs\\right)^{-\\frac{1}{2}}\\propto\\frac{1}{\\sqrt{cs}}\\exp{\\left\\{\\frac{1}{2\\sigma^2}\\left(\\frac{R^2\\left(\\omega\\right)}{c}+\\frac{I^2\\left(\\omega\\right)}{s}\\right)\\right\\}}$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally show that the MAP estimate of the frequency of your signal is obtained as the value of $\\omega$ which maximises the discrete Fourier tranform power spectrum of the signal (periodogram), $C(\\omega)$:<center>\n",
    "    $C(\\omega)=\\frac{2}{N}\\left\\lvert\\sum^N_{k=1}d_ke^{-i\\omega t_k}\\right\\rvert^2$\n",
    "</center>\n",
    "Summarize all the assumptions you made along the way to reach this result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the marginalized posterior just found we can always apply a log (which does not alter our MAP search):\n",
    "<center>\n",
    "$\\log{P(\\omega|D, I )}\\propto\\frac{R^2\\left(\\omega\\right)}{c}+\\frac{I^2\\left(\\omega\\right)}{s}$\n",
    "</center>\n",
    "since $s$ and $c$ are $\\omega$-independent for large $\\omega$ we can essentially treat them as constants in our maximisation endeavour; we can therefore discard them (after an eventual rescaling) to get a proportionality of:\n",
    "<center>\n",
    "$\\log{P(\\omega|D, I )}\\propto R^2\\left(\\omega\\right)+I^2\\left(\\omega\\right)\\propto \\frac{2}{N}\\left[R^2\\left(\\omega\\right)+I^2\\left(\\omega\\right)\\right]=\\frac{2}{N}\\left\\lvert R\\left(\\omega\\right)-iI^2\\left(\\omega\\right)\\right\\rvert^2=\\frac{2}{N}\\left\\lvert \\sum_{i=1}^Nd_i\\cos{\\omega t_i}-i\\sum_{i=1}^Nd_i\\sin{\\omega t_i}\\right\\rvert^2=$\n",
    "</center>\n",
    "<center>\n",
    "$=\\frac{2}{N}\\left\\lvert \\sum_{i=1}^Nd_i\\left[\\cos{\\omega t_i}-i\\sin{\\omega t_i}\\right]\\right\\rvert^2=\n",
    "\\frac{2}{N}\\left\\lvert \\sum_{k=1}^Nd_k\\left[\\frac{1}{2}\\left(e^{i\\omega t_k}+e^{-i\\omega t_k}-e^{i\\omega t_k}+e^{-i\\omega t_k}\\right)\\right]\\right\\rvert^2=\\frac{2}{N}\\left\\lvert\\sum^N_{k=1}d_ke^{-i\\omega t_k}\\right\\rvert ^2=C(\\omega)$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Under which condition on $\\omega$ the MAP estimate above would have been obtained via least square fitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating such frequency through least square fitting would have implied minimising the residual sum of squares between the measurements and the model, which reduces to the $Q$ parameter defined earlier:<center>\n",
    "    $\\sum_{i=1}^N\\left(d_i-B_1\\cos{\\omega t_i}-B_2\\sin{\\omega t_i}\\right)^2=\\sum_{i=1}^N\\left[d_i^2+B_1^2\\cos^2{\\omega t_i}+B_2^2\\sin^2{\\omega t_i}-2d_i\\left(B_1\\cos{\\omega t_i}+B_2\\sin{\\omega t_i}\\right)-2B_1B_2\\cos{\\omega t_i}\\sin{\\omega t_i}\\right]=$</center><center>\n",
    "\n",
    "    \n",
    "</center><center>\n",
    "$ =N\\hat{d}^2-2\\left[B_1R(\\omega)+B_2I(\\omega)\\right]+B_1^2c+B_2^2s=Q(B_1,B_2,\\omega)$\n",
    "</center><br>\n",
    "as seen before. Now we'd like to minimise it with respect to its parameters, $B_1,B_2,\\omega$:\n",
    "<center>\n",
    "$ 0=\\large\\frac{\\partial Q}{\\partial B_1}\\small=-2R(\\omega)+2B_1c\\implies B_1=\\large\\frac{R(\\omega)}{c}$\n",
    "    \n",
    "</center>\n",
    "<center>\n",
    "$ 0=\\large\\frac{\\partial Q}{\\partial B_2}\\small=-2I(\\omega)+2B_2s\\implies B_2=\\large\\frac{I(\\omega)}{s}$\n",
    "    \n",
    "</center><br>\n",
    "the second derivatives are clearly positive, yealding two minima; substituting them again in Q and minimising it with respect to $\\omega$ we get:\n",
    "<center>\n",
    "$ 0=\\large\\frac{\\partial Q}{\\partial \\omega}\\small=\\large\\frac{\\partial\\left\\{ N\\hat{d}^2-2\\left[\\frac{R^2(\\omega)}{c}+\\frac{I^2(\\omega)}{s}\\right]+\\frac{R^2(\\omega)}{c}+\\frac{I^2(\\omega)}{s}\\right\\}}{\\partial \\omega}\\small=\\large-\\frac{\\partial\\left(\\frac{R^2(\\omega)}{c}+\\frac{I^2(\\omega)}{s}\\right)}{\\partial \\omega}$\n",
    "    \n",
    "</center><br>\n",
    "minimising this is clearly equals to maximising $\\log{P(\\omega|D, I )}$, the MAP procedure. Such equivalence is only fair under the condition of $\\omega$ being very large. If this wasn't the case we would have not been able to make the simplifications that lead to both the least square parameter $Q$ and the posterior, as they would have had a term that goes like $B_1B_2\\cos{\\omega t_i}\\sin{\\omega t_i}$. Such term would have implied a mixed element in the marginalization of the posterior which would have broken the gaussian integrals computed and therefore made it impossible to get to a form of the posterior such as $\\left(\\frac{R^2\\left(\\omega\\right)}{c}+\\frac{I^2\\left(\\omega\\right)}{s}\\right)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
